import numpy as np
import jieba
import os
from rank_bm25 import BM25Okapi
from rouge_score import rouge_scorer
from sentence_transformers import SentenceTransformer, util
from FlagEmbedding import BGEM3FlagModel
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.decomposition import LatentDirichletAllocation

class FlowerNetVerifier:
    def __init__(self):
        print("ğŸŒ¸ FlowerNet éªŒè¯å±‚å¯åŠ¨ï¼ˆå»¶è¿ŸåŠ è½½æ¨¡å¼ï¼‰")
        
        # Verifier è‡ªå·±çš„å…¬ç½‘ URL
        self.public_url = os.getenv('VERIFIER_PUBLIC_URL', 'http://localhost:8000')
        print(f"  - Verifier Public URL: {self.public_url}")
        
        # å»¶è¿ŸåŠ è½½ï¼šæ¨¡å‹é¦–æ¬¡ä½¿ç”¨æ—¶æ‰åŠ è½½ï¼ˆèŠ‚çœå†…å­˜ï¼‰
        self._bge_model = None
        self._sbert_model = None
        
        # è½»é‡çº§ç»„ä»¶ç«‹å³åˆå§‹åŒ–
        self.scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)
        self.vectorizer = CountVectorizer(stop_words='english')
        print("âœ… éªŒè¯å±‚å°±ç»ªï¼ˆæ¨¡å‹å°†æŒ‰éœ€åŠ è½½ï¼‰")
    
    @property
    def bge_model(self):
        """å»¶è¿ŸåŠ è½½ BGE-M3 æ¨¡å‹ï¼ˆå¦‚æœå†…å­˜ä¸è¶³ï¼Œå›é€€åˆ° sbertï¼‰"""
        if self._bge_model is None:
            try:
                print("â³ å°è¯•åŠ è½½ BGE-M3 æ¨¡å‹...")
                # ä½¿ç”¨æ›´å°çš„æ¨¡å‹æˆ–ç›´æ¥ç”¨ sbert ä»£æ›¿
                # self._bge_model = BGEM3FlagModel('BAAI/bge-m3', use_fp16=True)
                # æ”¹ä¸ºä½¿ç”¨å·²æœ‰çš„ sbert_model æ¥èŠ‚çœå†…å­˜
                print("âš ï¸  å†…å­˜å—é™ï¼Œä½¿ç”¨ SentenceBERT ä»£æ›¿ BGE-M3")
                self._bge_model = self.sbert_model  # å¤ç”¨åŒä¸€ä¸ªæ¨¡å‹
                print("âœ… ä½¿ç”¨è½»é‡çº§æ¨¡å‹")
            except Exception as e:
                print(f"âŒ BGE-M3 åŠ è½½å¤±è´¥: {e}ï¼Œä½¿ç”¨ SentenceBERT")
                self._bge_model = self.sbert_model
        return self._bge_model
    
    @property
    def sbert_model(self):
        """å»¶è¿ŸåŠ è½½ SentenceBERT æ¨¡å‹"""
        if self._sbert_model is None:
            print("â³ é¦–æ¬¡åŠ è½½ SentenceBERT æ¨¡å‹...")
            self._sbert_model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')
            print("âœ… SentenceBERT å·²åŠ è½½")
        return self._sbert_model

    # --- æ ¸å¿ƒè¾…åŠ©å·¥å…· ---
    def _tokenize(self, text):
        """å¯¹ä¸­æ–‡æˆ–è‹±æ–‡è¿›è¡Œåˆ†è¯"""
        return list(jieba.cut(text))

    def _get_lda_topics(self, text, n_topics=1):
        """æå–æ–‡æœ¬çš„ä¸»é¢˜åˆ†å¸ƒç‰¹å¾"""
        try:
            words = [" ".join(self._tokenize(text))]
            tf = self.vectorizer.fit_transform(words)
            lda = LatentDirichletAllocation(n_components=n_topics, random_state=0)
            lda.fit(tf)
            return lda.components_
        except:
            return np.zeros((1, 10)) # å…œåº•å¤„ç†

    # --- ç»´åº¦ 1: ç›¸å…³æ€§æ£€æµ‹ (Relevancy) ---
    def calculate_relevancy(self, draft, outline):
        """
        è®¡ç®—å½“å‰èŠ±ç“£(Draft)ä¸èŠ±å¿ƒå¤§çº²(Outline)çš„ç›¸å…³æ€§
        æ”¹è¿›ç®—æ³•ï¼šå¤„ç†é•¿çŸ­æ–‡æœ¬çš„ç›¸å…³æ€§åˆ¤æ–­
        ç®—æ³•ç»„åˆï¼šå…³é”®è¯è¦†ç›–åº¦ + è¯­ä¹‰ç›¸ä¼¼åº¦ + ä¸»é¢˜ä¸€è‡´æ€§
        """
        # 1. å…³é”®è¯è¦†ç›–åº¦ (Keyword Coverage)
        # æå–å¤§çº²ä¸­çš„å…³é”®è¯ï¼ˆé•¿åº¦>2çš„è¯ï¼‰ï¼Œè®¡ç®—åœ¨è‰ç¨¿ä¸­çš„è¦†ç›–ç‡
        outline_tokens = [w for w in self._tokenize(outline) if len(w) > 1]
        draft_tokens = [w for w in self._tokenize(draft) if len(w) > 1]
        
        if outline_tokens:
            outline_keywords = set(outline_tokens)
            draft_keywords = set(draft_tokens)
            keyword_coverage = len(outline_keywords & draft_keywords) / len(outline_keywords)
        else:
            keyword_coverage = 0.0

        # 2. è¯­ä¹‰ç›¸ä¼¼åº¦ (Semantic Similarity)
        # ä½¿ç”¨ç»å¯¹å€¼æ¥è®¡ç®—ç›¸ä¼¼åº¦ï¼Œé¿å…åå‘å‘é‡å¯¼è‡´çš„è´Ÿå€¼é—®é¢˜
        emb_draft = self.sbert_model.encode(draft, convert_to_tensor=True)
        emb_outline = self.sbert_model.encode(outline, convert_to_tensor=True)
        semantic_sim_raw = util.pytorch_cos_sim(emb_draft, emb_outline).item()
        # ä½¿ç”¨ç»å¯¹å€¼ç¡®ä¿ç›¸å…³æ€§ä¸ºæ­£ï¼Œç„¶åå½’ä¸€åŒ–åˆ° [0, 1]
        semantic_sim = abs(semantic_sim_raw)

        # 3. ä¸»é¢˜ä¸€è‡´æ€§ (Topic Consistency)
        # æ£€æŸ¥å¤§çº²çš„æ ¸å¿ƒè¯æ±‡åœ¨è‰ç¨¿ä¸­çš„é‡å¤ç‡ï¼ˆè¯æ±‡å¯†åº¦ï¼‰
        # è®¡ç®—æ–¹å¼ï¼šå…³é”®è¯å‡ºç°æ¬¡æ•° / æ€»è¯æ•°
        outline_keywords_list = list(outline_keywords) if outline_tokens else []
        if outline_keywords_list and draft_tokens:
            keyword_frequency = sum(draft_tokens.count(kw) for kw in outline_keywords_list) / len(draft_tokens)
            # å½’ä¸€åŒ–ï¼šæœŸæœ›é¢‘ç‡çº¦ä¸º 0.1-0.3 ä¸ºæœ€ä¼˜
            topic_consistency = min(keyword_frequency / 0.2, 1.0)
        else:
            topic_consistency = 0.0

        # 4. æƒé‡èåˆ 
        # å…³é”®è¯è¦†ç›–åº¦ (0.4) + è¯­ä¹‰ç›¸ä¼¼åº¦ (0.4) + ä¸»é¢˜ä¸€è‡´æ€§ (0.2)
        total_relevancy = (keyword_coverage * 0.4) + (semantic_sim * 0.4) + (topic_consistency * 0.2)
        
        return {
            "score": float(round(total_relevancy, 4)),
            "details": {
                "keyword_coverage": float(round(keyword_coverage, 4)),
                "semantic_similarity": float(round(semantic_sim, 4)),
                "topic_consistency": float(round(topic_consistency, 4)),
            },
        }

    # --- ç»´åº¦ 2: å†—ä½™åº¦æ£€æµ‹ (Redundancy) ---
    def calculate_redundancy(self, draft, history_list):
        """
        æ£€æµ‹å½“å‰èŠ±ç“£(Draft)ä¸å·²ç”Ÿæˆçš„èŠ±ç“£(History)æ˜¯å¦é‡å¤
        ç®—æ³•ç»„åˆï¼šBGE-M3(é«˜ç²¾åº¦å»é‡) + LDA(ä¸»é¢˜é‡å¤) + FActScore(äº‹å®ç®€åŒ–ç‰ˆ)
        """
        if not history_list:
            return {"score": 0.0, "details": "No history yet"}

        # 1. è¯­ä¹‰ç›¸ä¼¼åº¦æ£€æµ‹ï¼ˆä½¿ç”¨ SentenceBERTï¼ŒèŠ‚çœå†…å­˜ï¼‰
        # ç›´æ¥ç”¨ sbert è€Œä¸æ˜¯ bgeï¼Œé¿å…åŠ è½½å¤§æ¨¡å‹
        all_histories = " ".join(history_list)
        
        emb_draft = self.sbert_model.encode(draft, convert_to_tensor=True)
        emb_history = self.sbert_model.encode(all_histories, convert_to_tensor=True)
        semantic_sim = util.pytorch_cos_sim(emb_draft, emb_history).item()

        # 2. ä¸»é¢˜é‡å¤æ£€æµ‹ï¼ˆå·²ç»è®¡ç®—è¿‡äº†ï¼Œå¤ç”¨ï¼‰
        topic_overlap = semantic_sim  # ä½¿ç”¨åŒæ ·çš„è¯­ä¹‰ç›¸ä¼¼åº¦

        # 3. äº‹å®å±‚é¢ç®€åŒ–æ£€æµ‹ (æ¨¡æ‹Ÿ FActScore é€»è¾‘)
        # æˆ‘ä»¬æ£€æµ‹ Draft ä¸­çš„æ ¸å¿ƒåè¯åœ¨ History ä¸­å‡ºç°çš„é¢‘ç‡
        draft_keywords = set([w for w in self._tokenize(draft) if len(w) > 1])
        history_keywords = set([w for w in self._tokenize(all_histories) if len(w) > 1])
        fact_overlap = len(draft_keywords & history_keywords) / max(len(draft_keywords), 1)

        # 4. æƒé‡èåˆï¼ˆç®€åŒ–ç‰ˆï¼Œä¸ä½¿ç”¨ BGEï¼‰
        # å†—ä½™å¾—åˆ†è¶Šé«˜ï¼Œè¯´æ˜è¶Šé‡å¤ã€‚ä½¿ç”¨è¯­ä¹‰ç›¸ä¼¼åº¦å’Œäº‹å®é‡å 
        total_redundancy = (semantic_sim * 0.6) + (fact_overlap * 0.4)

        return {
            "score": float(round(total_redundancy, 4)),
            "details": {
                "semantic": float(semantic_sim),
                "topic": float(topic_overlap),
                "fact": float(fact_overlap),
            },
        }

    # --- ç»´åº¦ 3: ç»¼åˆåˆ¤å®š (Decision) ---
    def verify(self, draft, outline, history_list, rel_threshold=0.4, red_threshold=0.6):
        """
        ä¸€é”®éªŒè¯é€»è¾‘ï¼šFlowerNet å†³å®šæ˜¯å¦è¿›å…¥ä¸‹ä¸€æ­¥
        """
        rel = self.calculate_relevancy(draft, outline)
        red = self.calculate_redundancy(draft, history_list)
        
        # åˆ¤å®šé€»è¾‘ï¼šç›¸å…³æ€§è¦é«˜ï¼Œå†—ä½™åº¦è¦ä½
        is_passed = (rel['score'] >= rel_threshold) and (red['score'] <= red_threshold)
        
        advice = "Content looks good."
        if rel['score'] < rel_threshold:
            advice = "Content is deviating from the outline. Add more focus on the section mission."
        if red['score'] > red_threshold:
            advice = "Content is redundant with previous sections. Provide new information."

        return {
            "is_passed": is_passed,
            "relevancy_index": rel['score'],
            "redundancy_index": red['score'],
            "feedback": advice,
            "raw_data": {"relevancy": rel['details'], "redundancy": red['details']}
        }

# --- æœ¬åœ°æµ‹è¯•ä»£ç  ---
if __name__ == "__main__":
    verifier = FlowerNetVerifier()
    test_outline = "Discuss the impact of AI on modern healthcare and medical diagnosis."
    test_draft = "AI has revolutionized modern healthcare and medical diagnosis by introducing unprecedented efficiency and accuracy. In medical diagnosis, AI-powered systems can analyze complex medical data, such as imaging scans, lab results, and electronic health records (EHRs), at a speed far exceeding human capabilities, enabling early detection of diseases like cancer, cardiovascular disorders, and neurological conditions. These systems reduce diagnostic errors caused by human fatigue or limited expertise, especially in regions with a shortage of specialized physicians. Beyond diagnosis, AI optimizes healthcare delivery by streamlining administrative tasks, personalizing treatment plans based on individual patient data, and predicting disease outbreaks, thereby improving overall healthcare accessibility and patient outcomes. While challenge"
    test_history = ["The history of healthcare starts from ancient times.", "Modern hospitals use digital records."]
    
    result = verifier.verify(test_draft, test_outline, test_history)
    print("\n--- FlowerNet éªŒè¯ç»“æœ ---")
    print(f"æ˜¯å¦é€šè¿‡: {result['is_passed']}")
    print(f"ç›¸å…³æ€§å¾—åˆ†: {result['relevancy_index']}")
    print(f"å†—ä½™åº¦å¾—åˆ†: {result['redundancy_index']}")
    print(f"æ§åˆ¶å±‚åé¦ˆ: {result['feedback']}")