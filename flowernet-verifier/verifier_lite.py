"""
æžç®€ç‰ˆ Verifier - æ— éœ€å¤§æ¨¡åž‹ï¼Œä»…ç”¨ä¼ ç»Ÿ NLP ç®—æ³•
ç”¨äºŽ Render å…è´¹ç‰ˆçš„å†…å­˜çº¦æŸçŽ¯å¢ƒ
"""

import numpy as np
import jieba
import os
from rank_bm25 import BM25Okapi
from rouge_score import rouge_scorer
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.decomposition import LatentDirichletAllocation


class FlowerNetVerifier:
    def __init__(self):
        print("ðŸŒ¸ FlowerNet éªŒè¯å±‚å¯åŠ¨ï¼ˆè½»é‡çº§æ¨¡å¼ï¼‰")
        
        # Verifier è‡ªå·±çš„å…¬ç½‘ URL
        self.public_url = os.getenv('VERIFIER_PUBLIC_URL', 'http://localhost:8000')
        print(f"  - Verifier Public URL: {self.public_url}")
        
        # è½»é‡çº§ç»„ä»¶
        self.scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)
        self.vectorizer = CountVectorizer(stop_words='english', max_features=1000)
        print("âœ… éªŒè¯å±‚å°±ç»ªï¼ˆä»…ä½¿ç”¨ä¼ ç»Ÿ NLPï¼‰")

    def _tokenize(self, text):
        """åˆ†è¯"""
        return list(jieba.cut(text))

    # --- ç»´åº¦ 1: ç›¸å…³æ€§æ£€æµ‹ ---
    def calculate_relevancy(self, draft, outline):
        """
        è®¡ç®—è‰ç¨¿ä¸Žå¤§çº²çš„ç›¸å…³æ€§
        ç®€åŒ–ç®—æ³•ï¼šå…³é”®è¯è¦†ç›– + BM25 ç›¸ä¼¼åº¦
        """
        # 1. å…³é”®è¯è¦†ç›–åº¦
        outline_tokens = [w for w in self._tokenize(outline) if len(w) > 1]
        draft_tokens = [w for w in self._tokenize(draft) if len(w) > 1]
        
        if outline_tokens:
            outline_keywords = set(outline_tokens)
            draft_keywords = set(draft_tokens)
            keyword_coverage = len(outline_keywords & draft_keywords) / len(outline_keywords)
        else:
            keyword_coverage = 0.0

        # 2. BM25 ç›¸ä¼¼åº¦
        try:
            bm25 = BM25Okapi([outline_tokens, draft_tokens])
            bm25_score = bm25.get_scores(draft_tokens)[0] / (bm25.get_scores(draft_tokens).max() + 1e-9)
        except:
            bm25_score = 0.5

        # 3. é•¿åº¦ç›¸å…³æ€§ï¼ˆä¸è¦å¤ªçŸ­ï¼‰
        length_score = min(len(draft) / max(len(outline), 1), 1.0)

        # ç»¼åˆç›¸å…³æ€§
        relevancy_score = (keyword_coverage * 0.4) + (bm25_score * 0.3) + (length_score * 0.3)

        return {
            "score": float(round(relevancy_score, 4)),
            "details": {
                "keyword_coverage": float(keyword_coverage),
                "bm25_similarity": float(bm25_score),
                "length_score": float(length_score),
            },
        }

    # --- ç»´åº¦ 2: å†—ä½™æ£€æµ‹ ---
    def calculate_redundancy(self, draft, history_list):
        """
        è®¡ç®—å†—ä½™åº¦
        ç®€åŒ–ç®—æ³•ï¼šè¯è¯­é‡å  + N-gram é‡å¤
        """
        if not history_list:
            return {"score": 0.0, "details": "No history yet"}

        all_histories = " ".join(history_list)
        draft_tokens = set(self._tokenize(draft))
        history_tokens = set(self._tokenize(all_histories))

        # 1. è¯è¯­é‡å åº¦
        if draft_tokens:
            token_overlap = len(draft_tokens & history_tokens) / len(draft_tokens)
        else:
            token_overlap = 0.0

        # 2. å¥å­çº§ N-gram é‡å¤ï¼ˆç®€å•ç‰ˆï¼‰
        draft_bigrams = set(zip(draft_tokens, list(draft_tokens)[1:]))
        history_bigrams = set(zip(history_tokens, list(history_tokens)[1:]))
        
        if draft_bigrams:
            bigram_overlap = len(draft_bigrams & history_bigrams) / len(draft_bigrams)
        else:
            bigram_overlap = 0.0

        # ç»¼åˆå†—ä½™åº¦
        redundancy_score = (token_overlap * 0.5) + (bigram_overlap * 0.5)

        return {
            "score": float(round(redundancy_score, 4)),
            "details": {
                "token_overlap": float(token_overlap),
                "bigram_overlap": float(bigram_overlap),
            },
        }

    # --- ç»´åº¦ 3: ç»¼åˆåˆ¤å®š ---
    def verify(self, draft, outline, history_list, rel_threshold=0.4, red_threshold=0.6):
        """
        ä¸€é”®éªŒè¯é€»è¾‘
        """
        rel = self.calculate_relevancy(draft, outline)
        red = self.calculate_redundancy(draft, history_list)

        # åˆ¤å®šé€»è¾‘
        is_passed = (rel['score'] >= rel_threshold) and (red['score'] <= red_threshold)

        advice = "Content looks good."
        if rel['score'] < rel_threshold:
            advice = "Content is deviating from the outline. Add more focus on the section mission."
        if red['score'] > red_threshold:
            advice = "Content is redundant with previous sections. Provide new information."

        return {
            "is_passed": is_passed,
            "relevancy_index": rel['score'],
            "redundancy_index": red['score'],
            "feedback": advice,
            "raw_data": {"relevancy": rel['details'], "redundancy": red['details']}
        }
